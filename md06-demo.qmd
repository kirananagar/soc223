---
title: "MD06 Demo, Part I"
format: html
embed-resources: true
editor: visual
editor_options: 
  chunk_output_type: console
---

Load packages and set theme.

```{r}
#| message: false
library(tidyverse)
library(moderndive)
library(ggthemes)
library(patchwork)
theme_set(theme_light())
```

Let's look at the MD evaluations data.

```{r}
data(evals)
glimpse(evals)
```

We are going to ask whether the "beauty" of an instructor predicts their teaching evaluations.

```{r}
d <- evals |>
  rename(bty = bty_avg,    # just shorter to type
         sex = gender)     # actually what they have

glimpse(d)
```

Let's look at the first few cases.

```{r}
head(d)
```

Here's the regression from last week.

```{r}
mod1 <- lm(score ~ bty,
           data = d)

get_regression_table(mod1)
```

Let's look at the predictions and residuals.

```{r}
mod1_preds <- get_regression_points(mod1) #this shows the predicted value from our regression and then the actual value from the data

head(mod1_preds)
```

Here's the regression line (blue). From the regression table, we can tell that when beauty = 0 (which does not actually mean anything in practice), evaluation would be 3.88. Then, for every "beauty point" it's another 0.067 evaluation points.

```{r,echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Simple regression results")
```

Here are the residuals.

```{r,echo=FALSE}
ggplot(mod1_preds,
       aes(x = bty,
           y = residual)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = 0,
             color = "red") +
  labs(x = "Beauty",
       y = "Residual",
       title = "Simple regression residuals")
```

One way to quantify how well a predictor predicts the outcome is to use the **variance**. We've seen this already but let's review. Here's the formula. $\bar{y}$ is the mean of $y$.

$$
V(y) = \frac{1}{n-1}\sum_{i=1}^{n}(y_i - \bar{y})^2
$$

Average squared distance between the observation and the mean. Regression is the study of variance, and how we interpret it.

Since our outcome is evaluation score, we can just do that.

```{r}
var_y <- d |> 
  pull(score) |> 
  var()

var_y
```

This is equivalent to the variance of the "residuals" when we just guess the mean value for every person!

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = mean(d$score),
             color = "blue") +
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Guessing the mean for everyone")
```

Adding `beauty` as a predictor improves our guess.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = mean(d$score),
             color = "blue") +
  geom_smooth(method = "lm",
              se = FALSE,
              color = "red",
              linetype = "dashed") +
  labs(x = "Beauty",
       y = "Evaluation",
       title = "Mean vs. regression line")

# Is the red line a better fit for the data than the blue mean line?
```

Central argument---I could just guess the average for everyone. Or, I could add a predictor to see if it improves our fit. This is regression.

Now let's see what the spread looks like if we look at the residuals from the regression line.

```{r}
var_yhat1 <- mod1_preds |> 
  pull(residual) |> 
  var()

var_yhat1
```

If we take the ratio of these and subtract it from one, that gives us the $R^2$ or "proportion of variance explained" by beauty scores.

```{r}
1 - (var_yhat1 / var_y)
```

It looks like "beauty" can account for about `r round((1 - (var_yhat1 / var_y)) * 100, 1)` percent of the variance in the evaluation scores.

In other words, if we try to guess instructors' evaluation scores, our guesses will be `r round((1 - (var_yhat1 / var_y)) * 100, 1)` percent *less wrong* on average if we know the instructor's beauty rating.

We can get this from `broom::glance()` or `moderndive::get_regression_summaries()` without doing it manually. The latter will be a more curated list of things you'll need for this course.

```{r}
broom::glance(mod1)
moderndive::get_regression_summaries(mod1)
max(d$bty) * 0.67 - min(d$bty) * 0.67
```

Let's try a different predictor. Let's predict ratings with `sex`.

```{r}
mod2 <- lm(score ~ sex,
           data = d)

get_regression_table(mod2)
get_regression_summaries(mod2)
```

Looks like male instructors get slightly better evaluations but this only accounts for a tiny bit of the of the variance. Don't be confused however: small amounts of variance can be really important even if they don't tell the whole story of variability.

In general, we are not going to want to do this with different variables one by one. We want to do **multiple regression**.

```{r}
mod3 <- lm(score ~ bty + sex,
           data = d)

get_regression_table(mod3)
```

Here's what this model does.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score,
           group = sex,
           color = sex)) +
  geom_jitter(alpha = .3) +
  geom_parallel_slopes(se = FALSE) +  # this is a modern dive thing!
  theme(legend.position = "top")
```

Here's the formula:

$$
\widehat{score}_i = 3.75 + .074(beauty_i) + .172(male_i)
$$

We might instead prefer a model like this, which allows the relationship between beauty and evaluations to be *different* for male and female instructors.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score,
           group = sex,
           color = sex)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",
              se = FALSE) +
  theme(legend.position = "top")
```

Here's the syntax, results, and formula.

```{r}
mod4 <- lm(score ~ bty + sex + bty:sex,
           data = d)

get_regression_table(mod4)
```

$$
\widehat{score}_i = 3.95 + .031(beauty_i) + -.184(male_i) + .080(beauty_i \times male_i)
$$ The slope for female instructors is .031. The slope for male instructors is .031 + .080 = .111.

```{r}
get_regression_summaries(mod3) # parallel
get_regression_summaries(mod4) # interactions
```

Do you think this is an important improvement?

## Now, We Practice!

```{r}
#| echo: false
ggplot(d,
       aes(x = age,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(x = "Age",
       y = "Evaluation",
       title = "Simple regression results")
```

Using a simple regression, it looks like there is a relationship between age and evaluation score. It seems to suggest that older instructors receive lower evaluation scores, in general.

Now, I'm going to get a regression so that I can plot my residuals.

```{r}
mod2 <- lm(score ~ age,
           data = d)
mod2
```

```{r}
mod2_preds <- get_regression_points(mod2)
head(mod2_preds)
```

```{r}
ggplot(mod2_preds,
       aes(x = age,
           y = residual)) +
  geom_jitter(alpha = .3) +
  geom_hline(yintercept = 0,
             color = "red") +
  labs(x = "Age",
       y = "Residual",
       title = "Simple regression residuals")
```

I don't think that there is any discernible pattern in the residuals, which seems to indicate that this regression is the right method for the data. Now, I'll be looking at the variance in order to try and see how 'helpful' age is as a predictor. From earlier, we know the variance of the score is 0.3, approximately.

```{r}
var_yhat2 <- mod2_preds |> 
  pull(residual) |> 
  var()

var_yhat2

1 - (var_yhat2 / var_y)
round((1 - (var_yhat2 / var_y)) * 100, 1)
```

From what we can see from the data, it appears that age accounts for 1.2% of the variance in evaluation score. Basically, by using age to make a prediction of evaluation score, then our guess will be on average 1.2% less bad than if we had not known age.
